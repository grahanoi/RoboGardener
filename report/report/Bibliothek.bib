@inproceedings{damianUsingFullyConvolutional2019,
  title = {Using {{Fully Convolutional Networks}} for {{Rumex Obtusifolius Segmentation}}, a {{Preliminary Report}}},
  booktitle = {2019 {{International Symposium ELMAR}}},
  author = {Damian, Schori and Thomas, Anken and Dejan, Šeatović},
  date = {2019-09},
  pages = {119--122},
  issn = {1334-2630},
  doi = {10.1109/ELMAR.2019.8918914},
  url = {https://ieeexplore.ieee.org/document/8918914/?arnumber=8918914},
  urldate = {2024-07-23},
  abstract = {Image segmentation of specific plants is an important task in precision farming. Several influences such as changing light, varying arrangement of leaves and similarly looking plants are challenging. We present a solution for segmenting individual Rumex obtusifolius plants out of complicated natural scenes in grassland from 2D images. We are making use of a fully convolutional deep neural network (FCN) trained with hand labeled images. The proposed segmentation scheme is validated with images taken under outdoor conditions. The overall masks segmentation rate is 84.8\% measured by the dice coefficient. Approximately half of the experiments show segmentation rates of individual plants higher than 88\%. The developed solution is therefore a robust method to segment Rumex obtusifolius plants under real-world conditions in short time.},
  eventtitle = {2019 {{International Symposium ELMAR}}},
  keywords = {Computer Vision,Deep Learning,Image segmentation,Image Segmentation,Object segmentation,Real-time systems,Robots,Robustness,Rumex obtusifolius,Three-dimensional displays,Training},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\N5NJURKD\\Damian et al. - 2019 - Using Fully Convolutional Networks for Rumex Obtus.pdf;C\:\\Users\\noiri\\Zotero\\storage\\3EX4GQ5J\\8918914.html}
}

@online{dejanHowControlStepper2015,
  title = {How {{To Control}} a {{Stepper Motor}} with {{A4988 Driver}} and {{Arduino}}},
  author = {Dejan},
  date = {2015-08-16T11:01:33+00:00},
  url = {https://howtomechatronics.com/tutorials/arduino/how-to-control-stepper-motor-with-a4988-driver-and-arduino/},
  urldate = {2024-08-23},
  abstract = {In this Arduino Tutorial we will learn how to control a Stepper Motor using the A4988 Stepper Driver. The A4988 is a microstepping driver for controlling bipolar stepper motors which has built-in translator for easy operation. This means that we can control the stepper motor with...},
  langid = {american},
  organization = {How To Mechatronics},
  file = {C:\Users\noiri\Zotero\storage\RIS89YKB\how-to-control-stepper-motor-with-a4988-driver-and-arduino.html}
}

@thesis{dollingerKameragefuehrtePositionierungUnd2014,
  type = {Thesis},
  title = {Kamerageführte Positionierung und Greifbewegung eines Roboterarms},
  author = {Dollinger, Moritz},
  date = {2014-11-13},
  institution = {Hochschule für angewandte Wissenschaften Hamburg},
  url = {https://reposit.haw-hamburg.de/handle/20.500.12738/6774},
  urldate = {2024-03-06},
  abstract = {Ziel dieser Arbeit ist die bildbasierte Detektion und Rekonstruktion der Positionen von  Objekten im 3D-Raum für eine visuelle Steuerung eines Knickarmroboters. Dieser  soll die detektierten Gegenstände anfahren, greifen und der Größe nach sortieren.  Die Aufnahmen des Arbeitsraums des Roboterarms sollen aus verschiedenen  Perspektiven mit Hilfe zweier Kameras gewonnen werden. Durch einen einmaligen  Kalibriervorgang sollen die Positionen und die internen Parameter der verwendeten  Kameras zunächst bestimmt werden. Mit diesen Parametern können dann aus den  2D-Pixelkoordinaten der Objekte und des Greifers des Roboterarms die jeweiligen  Positionen im 3D-Raum rekonstruiert werden. Anschließend soll eine Bahnplanung  für den Greifer unter Verwendung der inversen Kinematik erfolgen, um die einzelnen  Objekte zu bewegen.},
  langid = {ngerman},
  annotation = {Accepted: 2020-09-29T12:46:13Z},
  file = {C:\Users\noiri\Zotero\storage\4NDUGRAC\Dollinger - 2014 - Kamerageführte Positionierung und Greifbewegung ei.pdf}
}

@video{engineeringeducatoracademyImageBasedVisualServoing2021,
  entrysubtype = {video},
  title = {Image-{{Based Visual Servoing}} - {{Robot Control Part}}  1},
  editor = {{Engineering Educator Academy}},
  editortype = {director},
  date = {2021-12-19},
  url = {https://www.youtube.com/watch?v=s_0DdxHjrfA},
  urldate = {2024-03-06},
  abstract = {Basics of computer-vision for image-based control of robotic arms are explained in this video, including the pinhole camera model, perspective projection, intrinsic and extrinsic camera parameters, focal axis and focal length, feature points and feature matching, and difference between position-based visual servo control and image-based visual servo control of robotic arms.}
}

@online{FarmBotOpenSourceCNC,
  title = {{{FarmBot}} | {{Open-Source CNC Farming}}},
  url = {https://farm.bot/},
  urldate = {2024-09-26},
  abstract = {Farming and gardening robots for home, educational, and commercial use. Premium Hardware · Worldwide Shipping · Drag and Drop Farm Designer · Step-by-Step Assembly Instructions · Own Your Food},
  langid = {english},
  organization = {FarmBot},
  file = {C:\Users\noiri\Zotero\storage\SFXPGFX3\farm.bot.html}
}

@online{GettingStartedGazebo,
  title = {Getting {{Started}} with {{Gazebo}}? — {{Gazebo}} Harmonic Documentation},
  url = {https://gazebosim.org/docs/latest/getstarted/},
  urldate = {2024-09-21},
  file = {C:\Users\noiri\Zotero\storage\QV8PSCHA\getstarted.html}
}

@online{GettingStartedImage,
  title = {Getting {{Started}} with {{Image Preprocessing}} in {{Python}}},
  url = {https://kaggle.com/code/rimmelasghar/getting-started-with-image-preprocessing-in-python},
  urldate = {2024-04-22},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from Animals-10},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\CF8WE6PC\getting-started-with-image-preprocessing-in-python.html}
}

@inproceedings{gobeeVisionbasedPillBlister2023,
  title = {Vision-Based {{Pill Blister Package Inspection System}} Using {{CNN}}},
  booktitle = {Proceedings of the 2023 13th {{International Conference}} on {{Biomedical Engineering}} and {{Technology}}},
  author = {Gobee, Suresh and Durairajah, Vickneswari and Prea, Laurent Eddie Sylvester},
  date = {2023-12-19},
  series = {{{ICBET}} '23},
  pages = {93--98},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3620679.3620694},
  url = {https://dl.acm.org/doi/10.1145/3620679.3620694},
  urldate = {2024-03-03},
  abstract = {This research aimed to develop an automated blister package sorting system based on defects detected using image data. The proposed approach aims to reduce the lead time incurred by manual sorting in the production environment and provide a low-cost automated alternative. The image detection model to detect the defects was developed using YOLOv5 pre-trained object detection model from the PyTorch framework. The dataset collected 350 labeled images, including 40\% of those which were augmented and duplicated using Roboflow image processing. The object detection results were leveraged in multiple ways including detecting the presence of blister package in the frame and for pose estimation, among the mainstream defect detection. The image coordinate was converted to a robot arm coordinate to enable effective manipulation. Dobot Magician Robot arm was used to actuate the blister package and was controlled using its native API. The image processing and robot arm manipulation module was integrated using Python. The detection model held a mean average precision(mAP) of 86.1\% for any random rotations of the blister package, and the sorting rate averaged 7.68s per iteration..},
  isbn = {9798400707438},
  keywords = {Deep learning,Machine Vision,Pill inspection,Robot arm integration}
}

@online{InstallingGazeboROS,
  title = {Installing {{Gazebo}} with {{ROS}} — {{Gazebo}} Harmonic Documentation},
  url = {https://gazebosim.org/docs/harmonic/ros_installation/#summary-of-compatible-ros-and-gazebo-combinations},
  urldate = {2024-09-20},
  file = {C:\Users\noiri\Zotero\storage\9ATSTMZZ\ros_installation.html}
}

@article{latschOptimisationHotwaterApplication2014,
  title = {Optimisation of Hot-Water Application Technology for the Control of Broad-Leaved Dock ({{Rumex}} Obtusifolius)},
  author = {Latsch, Roy and Sauter, Joachim},
  date = {2014-12-21},
  journaltitle = {Journal of Agricultural Engineering},
  volume = {45},
  number = {4},
  pages = {137--145},
  issn = {2239-6268},
  doi = {10.4081/jae.2014.239},
  url = {https://www.agroengineering.org/jae/article/view/jae.2014.239},
  urldate = {2024-07-23},
  abstract = {In organic farming, the control of broad-leaved dock (Rumex obtusifolius) via hot-water treatment of the upper root region (hypocotyl) is a new alternative to the current standard control method involving manual digging-out of the roots. This comparative study looks at five different hot-water application techniques. The aim is to optimise the control method in terms of water and energy requirement to obtain a mortality rate of the treated plants of at least 80\%. The studied parameters were the application, the amount of water, the water temperature, the soil moisture content and the soil type. In total, 813 plants of varying size were treated (120-225 plants per treatment). The success of each treatment was rated 12 weeks after it was applied. Based on the results, the preferred treatment in terms of water and energy requirement was a commercially available rotary nozzle. With this nozzle, for example, at 40 vol.-\% soil moisture, 1.6 L of water at a temperature of 90Â°C was necessary for successful Rumex control. The rotary nozzle could be used as a non-contact system, and was therefore the most user-friendly of the application techniques examined.},
  issue = {4},
  langid = {english},
  keywords = {broad-leaved dock,organic farming.,Rumex obtusifolius,thermal treatment,weed control},
  file = {C:\Users\noiri\Zotero\storage\L7C7U679\Latsch und Sauter - 2014 - Optimisation of hot-water application technology f.pdf}
}

@inproceedings{leDigitalTwinApproach2024,
  title = {Digital Twin Approach for Machining with Robotic Manipulator},
  booktitle = {Proceedings of the 2024 {{International Conference}} on {{Advanced Robotics}}, {{Automation Engineering}} and {{Machine Learning}}},
  author = {Le, Van and Mao, Xuanyu and Tran, Minh and Ding, Songlin},
  date = {2024-06-28},
  pages = {12--17},
  publisher = {ACM},
  location = {Hangzhou China},
  doi = {10.1145/3677454.3677457},
  url = {https://dl.acm.org/doi/10.1145/3677454.3677457},
  urldate = {2024-09-18},
  eventtitle = {{{ARAEML}} 2024: 2024 {{International Conference}} on {{Advanced Robotics}}, {{Automation Engineering}} and {{Machine Learning}}},
  isbn = {9798400717116},
  langid = {english}
}

@inproceedings{liuPerformanceValidationYolo2021,
  title = {Performance {{Validation}} of {{Yolo Variants}} for {{Object Detection}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Bioinformatics}} and {{Intelligent Computing}}},
  author = {Liu, Kaiyue and Tang, Haitong and He, Shuang and Yu, Qin and Xiong, Yulong and Wang, Nizhuan},
  date = {2021-03-21},
  series = {{{BIC}} 2021},
  pages = {239--243},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3448748.3448786},
  url = {https://dl.acm.org/doi/10.1145/3448748.3448786},
  urldate = {2024-03-04},
  abstract = {Object detection is a core part of an intelligent surveillance system and a fundamental algorithm in the field of identity identification, which is of great practical importance. Since the YOLO series algorithms have good results in terms of accuracy and speed, YOLO and each subsequent version have been surpassing. Thus, in this paper, it carries out experiments on three versions of popular YOLO models such as yolov3, yolov4, and yolov5 (yolov5l, yolov5m, yolov5s, yolov5x). The performance of the three versions of YOLO model is analyzed and summarized by training and predicting the public VOC dataset. Results showed that the yolov4 model is higher than the yolov3 model in terms of mAP values, but slightly lower in terms of speed, while the yolov5 series model is better than the yolov3 and yolov4 models both in terms of mAP values and speed.},
  isbn = {978-1-4503-9000-2},
  keywords = {Deep Learning,Object Detection,PASCAL VOC Dataset,YOLO},
  file = {C:\Users\noiri\Zotero\storage\29WYQF8T\Liu et al. - 2021 - Performance Validation of Yolo Variants for Object.pdf}
}

@inproceedings{luoVisionbased3objectPickplace2017,
  title = {Vision-Based 3-{{D}} Object Pick-and-Place Tasks of Industrial Manipulator},
  booktitle = {2017 {{International Automatic Control Conference}} ({{CACS}})},
  author = {Luo, Guor-Yieh and Cheng, Ming-Yang and Chiang, Chia-Ling},
  date = {2017-11},
  pages = {1--7},
  doi = {10.1109/CACS.2017.8284250},
  url = {https://ieeexplore.ieee.org/abstract/document/8284250},
  urldate = {2024-03-03},
  abstract = {When dealing with complicated tasks such as object pick-and-place, it is harder for robotic arms alone to complete them. One of the possible solutions to overcoming the aforementioned difficulties is to introduce machine vision into the robotic arm system. In this paper, the eye-to-hand camera configuration is adopted in the development of the vision-based automatic pick-and-place systems for 3-D objects. The vision-based automatic pick-and-place system developed in this paper consists of three main sections - calibration of machine vision system, object recognition, and transformations of object coordinates. Experimental results indicate that the vision-based automatic pick-and-place system developed in this paper is able to perform an automatic pick-and-place task for 3-D objects.},
  eventtitle = {2017 {{International Automatic Control Conference}} ({{CACS}})},
  keywords = {Calibration,Cameras,Machine vision,Manipulators,Object recognition,Objection recognition,Robot kinematics,Robot vision systems,Stereo vision},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\2DDJPEXL\\Luo et al. - 2017 - Vision-based 3-D object pick-and-place tasks of in.pdf;C\:\\Users\\noiri\\Zotero\\storage\\HMWKDSGC\\8284250.html}
}

@article{redmonYOLOv3IncrementalImprovement,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\NGSIBB5F\Redmon und Farhadi - YOLOv3 An Incremental Improvement.pdf}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-06},
  pages = {779--788},
  publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.91},
  url = {http://ieeexplore.ieee.org/document/7780460/},
  urldate = {2024-03-04},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  keywords = {Methods},
  file = {C:\Users\noiri\Zotero\storage\53C78TH7\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@online{ROS2Part122024,
  title = {{{ROS2 Part}} 12 - {{ROS2 Digital Twin}}},
  date = {2024-05-21T04:35:21+02:00},
  url = {https://www.roboticsunveiled.com/ros2-ros2-digital-twin/},
  urldate = {2024-08-15},
  abstract = {In this post we present an overview on the concept of ROS2 Digital Twins.},
  langid = {american},
  file = {C:\Users\noiri\Zotero\storage\X6QWJYAU\ros2-ros2-digital-twin.html}
}

@online{ROSHome,
  title = {{{ROS}}: {{Home}}},
  url = {https://www.ros.org/},
  urldate = {2024-08-14},
  file = {C:\Users\noiri\Zotero\storage\MF7NDGAH\www.ros.org.html}
}

@online{SDFormatHome,
  title = {{{SDFormat Home}}},
  url = {http://sdformat.org/},
  urldate = {2024-09-30},
  file = {C:\Users\noiri\Zotero\storage\2QMHYAQR\sdformat.org.html}
}

@online{WhatCloudComputing2023,
  title = {What {{Is Cloud Computing}}? | {{IBM}}},
  shorttitle = {What {{Is Cloud Computing}}?},
  date = {2023-04-25T00:00:00.000},
  url = {https://www.ibm.com/topics/cloud-computing},
  urldate = {2024-08-16},
  abstract = {Cloud computing enables customers to use infrastructure and applications by way of the internet, without installing and maintaining them on premises.},
  langid = {american}
}

@online{WhatEdgeComputing,
  title = {What {{Is Edge Computing}}? {{Everything You Need}} to {{Know}}},
  shorttitle = {What {{Is Edge Computing}}?},
  url = {https://www.techtarget.com/searchdatacenter/definition/edge-computing},
  urldate = {2024-08-16},
  abstract = {Learn about edge computing, how it works and the importance of its role in the growth of 5G. Discover why edge computing matters, including benefits and use cases.},
  langid = {english},
  organization = {Data Center},
  file = {C:\Users\noiri\Zotero\storage\HV28A3NB\edge-computing.html}
}
